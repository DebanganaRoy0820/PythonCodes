#Given an integer n, return an array ans of length n + 1 such that for each i (0 <= i <= n), ans[i] is the number of 1's in the binary representation of i. 
#Example 1:
#Input: n = 2
#Output: [0,1,1]
#Explanation:
#0 --> 0
#1 --> 1
#2 --> 10
def countBits(self, n: int) -> List[int]:
        sol=[0] * (n+1)
        for i in range(1,n+1):
            sol[i]=sol[i&(i-1)]+1
        return sol
